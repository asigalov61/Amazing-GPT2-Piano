{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "Chordified GPT2 Piano.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asigalov61/Amazing-GPT2-Piano/blob/master/Chordified_GPT2_Piano.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QA0W-VK1JVQl"
      },
      "source": [
        "# Chordified GPT2 Piano\n",
        "\n",
        "***\n",
        "\n",
        "## Chordified GPT2-based Symbolic Music Artificial Intelligence Model Creator/Trainer.\n",
        "\n",
        "### Multi-Instrumental, w/MIDI.py v.6.7 by Peter Billam\n",
        "\n",
        "***\n",
        "\n",
        "Credit for char-based GPT2 implementation used in this colab goes out to Andrej Karpathy: https://github.com/karpathy/minGPT\n",
        "\n",
        "***\n",
        "\n",
        "#### Project Los Angeles\n",
        "\n",
        "#### Tegridy Code 2020\n",
        "\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eftzIVKrqR5S"
      },
      "source": [
        "# Setup Environment, clone needed repos, and install all required dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsUtsJGNz6f2",
        "cellView": "form"
      },
      "source": [
        "#@title Clone minGPT repo and install all dependencies (run only once per session)\n",
        "!git clone https://github.com/asigalov61/minGPT\n",
        "!pip install pyknon\n",
        "!pip install pretty_midi\n",
        "!pip install pypianoroll\n",
        "!pip install mir_eval\n",
        "!apt install fluidsynth #Pip does not work for some reason. Only apt works\n",
        "!pip install midi2audio\n",
        "!cp /usr/share/sounds/sf2/FluidR3_GM.sf2 /content/font.sf2\n",
        "\n",
        "!curl -L \"https://github.com/asigalov61/MIDI-TXT-MIDI/raw/master/MIDI.py\" > 'MIDI.py'\n",
        "\n",
        "!mkdir Dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pf8B3p6QySmE",
        "cellView": "form"
      },
      "source": [
        "#@title Import all modules and setup logging\n",
        "%cd /content/minGPT\n",
        "# make deterministic\n",
        "from mingpt.utils import set_seed\n",
        "set_seed(42)\n",
        "\n",
        "\n",
        "import os\n",
        "import tqdm.auto\n",
        "\n",
        "import numpy as np\n",
        "import torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "import keras\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "import time\n",
        "import math\n",
        "\n",
        "from mingpt.model import GPT, GPTConfig\n",
        "from mingpt.trainer import Trainer, TrainerConfig\n",
        "from mingpt.utils import sample\n",
        "\n",
        "import tqdm.auto\n",
        "\n",
        "# For plotting\n",
        "import mido\n",
        "import librosa\n",
        "import pretty_midi\n",
        "import pypianoroll\n",
        "from pypianoroll import Multitrack, Track\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import mir_eval.display\n",
        "import librosa.display\n",
        "%matplotlib inline\n",
        "\n",
        "from mido import MidiFile\n",
        "\n",
        "\n",
        "from midi2audio import FluidSynth\n",
        "\n",
        "from google.colab import output, drive\n",
        "\n",
        "from IPython.display import display, Javascript, HTML, Audio, Image\n",
        "\n",
        "# set up logging\n",
        "import logging\n",
        "logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
        "        level=logging.INFO,\n",
        ")\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Assume that we are on a CUDA machine, then this should print a CUDA device:\n",
        "print('Available Processing Device is:', device)\n",
        "%cd /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2Pv5eNRqiyr"
      },
      "source": [
        "# Upload/download and process MIDI dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHuggjW7etzZ",
        "cellView": "form"
      },
      "source": [
        "#@title (OPTION 1) Download Tegridy special Piano/Violin MIDI dataset\n",
        "%cd /content/Dataset/\n",
        "!rm *.mid \n",
        "!rm *.midi\n",
        "!wget 'https://github.com/asigalov61/Tegridy-MIDI-Dataset/raw/master/Tegridy-MIDI-Dataset-CC-BY-NC-SA.zip'\n",
        "!unzip '/content/Dataset/Tegridy-MIDI-Dataset-CC-BY-NC-SA.zip'\n",
        "!rm '/content/Dataset/Tegridy-MIDI-Dataset-CC-BY-NC-SA.zip'\n",
        "%cd /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "XeLiUy4khu27"
      },
      "source": [
        "#@title (OPTION 2) Tiny Karaoke Precision MIDI subset (CC-BY-NC-SA)\n",
        "%cd /content/Dataset/\n",
        "!rm *.mid \n",
        "!rm *.midi\n",
        "!wget 'https://github.com/asigalov61/Tegridy-MIDI-Dataset/raw/master/Tiny-Karaoke-Precision-MIDI-Subset-CC-BY-NC-SA.zip'\n",
        "!unzip '/content/Dataset/Tiny-Karaoke-Precision-MIDI-Subset-CC-BY-NC-SA.zip'\n",
        "!rm '/content/Dataset/Tiny-Karaoke-Precision-MIDI-Subset-CC-BY-NC-SA.zip'\n",
        "%cd /content/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A378EUz_7K0G",
        "cellView": "form"
      },
      "source": [
        "#@title Process MIDI to TXT (w/MIDI.py v.6.7 by Peter Billam)\n",
        "debug = False #@param {type:\"boolean\"}\n",
        "chord_sampling_window_in_ms = 5 #@param {type:\"slider\", min:1, max:100, step:1}\n",
        "\n",
        "# MIDI Dataset to txt dataset converter \n",
        "import MIDI\n",
        "import os\n",
        "\n",
        "if os.path.exists(\"Dataset.txt\"):\n",
        "  os.remove(\"Dataset.txt\")\n",
        "  print('Removing old Dataset...')\n",
        "else:\n",
        "  print(\"Creating new Dataset file...\")\n",
        "\n",
        "dataset_addr = \"Dataset\"\n",
        "files = os.listdir(dataset_addr)\n",
        "for file in tqdm.auto.tqdm(files):\n",
        "    path = os.path.join(dataset_addr, file)\n",
        "\n",
        "    u = 0\n",
        "    score = []\n",
        "    unsorted_score = []\n",
        "    file_name = ''\n",
        "    chord_average_duration = 0\n",
        "    chord_average_velocity = 0\n",
        "\n",
        "    midi_file = open(path, 'rb')\n",
        "    if debug: print('Processing File:', file_address)\n",
        "    \n",
        "    score = MIDI.midi2score(midi_file.read())\n",
        "    #score = unsorted_score[1]\n",
        "    #score.sort(key=lambda x: x[1])\n",
        "\n",
        "    if debug: print(score)\n",
        "    file_name = os.path.basename(path)\n",
        "    \n",
        "    midi_file.close()\n",
        "    if debug: print(score)\n",
        "\n",
        "    itrack = 1\n",
        "\n",
        "    previous_event = ['note', 0, 0, 0, 0, 0]\n",
        "    chord = [['note', 0, 0, 0, 0, 0]]\n",
        "\n",
        "    this_channel_has_note = False\n",
        "\n",
        "    file = open('Dataset.txt', 'a')\n",
        "    file.write('SONG=' + str(file_name).replace(' ', '_')  + ' ')\n",
        "\n",
        "    #for itrack in range(len(score)):\n",
        "    #      event = score[itrack]\n",
        "    while itrack < len(score):\n",
        "      for event in score[itrack]:\n",
        "        if event[0] == 'note':\n",
        "          \n",
        "\n",
        "          this_channel_has_note = True\n",
        "\n",
        "          if previous_event[1] == 0:\n",
        "            previous_event = event\n",
        "\n",
        "          if previous_event[1] in range(event[1], event[1]+chord_sampling_window_in_ms):\n",
        "            chord.append(event)\n",
        "            chord_average_duration = int((previous_event[2] + event[2]) / 2)\n",
        "            chord_average_velocity = int((previous_event[5] + event[5]) / 2)\n",
        "          \n",
        "          else: \n",
        "            #if int(str(song_chord_header.split('=')[1]).split('-')[0]) <= int(str(chord[0][1])):\n",
        "            file.write('C=' + str(chord[0][1]) + '-' + str(chord_average_duration) + '-' + str(chord[0][3]) + '-' + str(chord_average_velocity) + ' N')\n",
        "\n",
        "            for notes in chord:\n",
        "              file.write('-' + str(notes[4]))\n",
        "            \n",
        "            file.write(' ')\n",
        "            \n",
        "            previous_event = event\n",
        "            if debug: print(chord)\n",
        "            chord = []\n",
        "            chord.append(previous_event)   \n",
        "          \n",
        "          if not this_channel_has_note:\n",
        "            this_channel_has_note = False\n",
        "            u+=1\n",
        "            if debug: print('Uknown Event: ', event[0])\n",
        "      itrack += 1\n",
        "\n",
        "    file.close()\n",
        "    if debug:\n",
        "      print('File:', midi_file, 'Number of skipped events: ', u)\n",
        "print('Done!')\n",
        "print('Number of skipped events: ', u)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cj2xl5xqwea"
      },
      "source": [
        "# Setup and Intialize the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1MQDOVMySmJ",
        "cellView": "form"
      },
      "source": [
        "#@title Setup functions and procedures\n",
        "model_attention_span_in_tokens = 256 #@param {type:\"slider\", min:0, max:512, step:16}\n",
        "\n",
        "class CharDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, block_size):\n",
        "        chars = sorted(list(set(data)))\n",
        "        data_size, vocab_size = len(data), len(chars)\n",
        "        print('data has %d characters, %d unique.' % (data_size, vocab_size))\n",
        "        \n",
        "        self.stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "        self.itos = { i:ch for i,ch in enumerate(chars) }\n",
        "        self.block_size = block_size\n",
        "        self.vocab_size = vocab_size\n",
        "        self.data = data\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.block_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # grab a chunk of (block_size + 1) characters from the data\n",
        "        chunk = self.data[idx:idx + self.block_size + 1]\n",
        "        # encode every character to an integer\n",
        "        dix = [self.stoi[s] for s in chunk]\n",
        "        \"\"\"\n",
        "        arrange data and targets so that the first i elements of x\n",
        "        will be asked to predict the i-th element of y. Notice that\n",
        "        the eventual language model will actually make block_size\n",
        "        individual predictions at the same time based on this data,\n",
        "        so we are being clever and amortizing the cost of the forward\n",
        "        pass of the network. So for example if block_size is 4, then\n",
        "        we could e.g. sample a chunk of text \"hello\", the integers in\n",
        "        x will correspond to \"hell\" and in y will be \"ello\". This will\n",
        "        then actually \"multitask\" 4 separate examples at the same time\n",
        "        in the language model:\n",
        "        - given just \"h\", please predict \"e\" as next\n",
        "        - given \"he\" please predict \"l\" next\n",
        "        - given \"hel\" predict \"l\" next\n",
        "        - given \"hell\" predict \"o\" next\n",
        "        \n",
        "        In addition, because the DataLoader will create batches of examples,\n",
        "        every forward/backward pass during traning will simultaneously train\n",
        "        a LOT of predictions, amortizing a lot of computation. In particular,\n",
        "        for a batched input of integers X (B, T) where B is batch size and\n",
        "        T is block_size and Y (B, T), the network will during training be\n",
        "        simultaneously training to make B*T predictions, all at once! Of course,\n",
        "        at test time we can paralellize across batch B, but unlike during training\n",
        "        we cannot parallelize across the time dimension T - we have to run\n",
        "        a forward pass of the network to recover the next single character of the \n",
        "        sequence along each batch dimension, and repeatedly always feed in a next\n",
        "        character to get the next one.\n",
        "        \n",
        "        So yes there is a big asymmetry between train/test time of autoregressive\n",
        "        models. During training we can go B*T at a time with every forward pass,\n",
        "        but during test time we can only go B at a time, T times, with T forward \n",
        "        passes.\n",
        "        \"\"\"\n",
        "        x = torch.tensor(dix[:-1], dtype=torch.long)\n",
        "        y = torch.tensor(dix[1:], dtype=torch.long)\n",
        "        return x, y\n",
        "\n",
        "        \n",
        "block_size = model_attention_span_in_tokens # spatial extent of the model for its context"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m4QIgbe3ySmN",
        "cellView": "form"
      },
      "source": [
        "#@title Specify input text file with training data (do not worry, any text format is fine)\n",
        "full_path_to_training_text_file = \"/content/Dataset.txt\" #@param {type:\"string\"}\n",
        "text = open(full_path_to_training_text_file, 'r').read() # don't worry we won't run out of file handles\n",
        "train_dataset = CharDataset(text, block_size) # one line of poem is roughly 50 characters"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpNxwzNkySmQ",
        "cellView": "form"
      },
      "source": [
        "#@title Create GPT2 model\n",
        "model_embed_size = 256 #@param {type:\"slider\", min:0, max:1024, step:64}\n",
        "number_of_heads = 16 #@param {type:\"slider\", min:1, max:16, step:1}\n",
        "number_of_layers = 4 #@param {type:\"slider\", min:1, max:16, step:1}\n",
        "\n",
        "\n",
        "mconf = GPTConfig(train_dataset.vocab_size, \n",
        "                  train_dataset.block_size,\n",
        "                  n_layer=number_of_layers, \n",
        "                  n_head=number_of_heads, \n",
        "                  n_embd=model_embed_size)\n",
        "\n",
        "model = GPT(mconf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWaOS0sRySmS",
        "cellView": "form"
      },
      "source": [
        "#@title Setup all training parameters\n",
        "number_of_training_epochs = 5 #@param {type:\"slider\", min:1, max:100, step:1}\n",
        "training_batch_size = 160 #@param {type:\"slider\", min:0, max:160, step:4}\n",
        "model_learning_rate = 6e-4 #@param {type:\"number\"}\n",
        "# initialize a trainer instance and kick off training\n",
        "\n",
        "tconf = TrainerConfig(max_epochs=number_of_training_epochs, \n",
        "                      batch_size=training_batch_size, \n",
        "                      learning_rate=model_learning_rate,\n",
        "                      num_workers=4)\n",
        "trainer = Trainer(model, train_dataset, None, tconf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_18H-M-q4CB"
      },
      "source": [
        "# Train the Model or load the existing pre-trained model checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRffqT9WFBHB",
        "cellView": "form"
      },
      "source": [
        "#@title (OPTION 1) Train the model\n",
        "%cd /content/\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVWEhUj1cg7N",
        "cellView": "form"
      },
      "source": [
        "#@title Plot Positional Embeddings\n",
        "\n",
        "# visualize some of the learned positional embeddings, maybe they contain structure\n",
        "plt.figure(figsize=(18, 1))  \n",
        "ci = model.pos_emb.data[0, :, 0].cpu()\n",
        "zci = torch.cat((torch.tensor([0.0]), ci)) # pre-cat a zero\n",
        "plt.imshow(zci.view(1, block_size+1).numpy())\n",
        "plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMkyEMghC-KR",
        "cellView": "form"
      },
      "source": [
        "#@title Save/Re-Save the model from memory\n",
        "%cd /content/\n",
        "torch.save(model, 'trained-model.pth')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CmD7VRZhDcnJ",
        "cellView": "form"
      },
      "source": [
        "#@title (OPTION 2) Load existing model/checkpoint\n",
        "full_path_to_model_checkpoint = \"/content/trained-model.pth\" #@param {type:\"string\"}\n",
        "model = torch.load(full_path_to_model_checkpoint)\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfgeQl8_rEox"
      },
      "source": [
        "# Generate, download, plot, and listen to the output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZEqKJ6NySmV",
        "cellView": "form"
      },
      "source": [
        "#@title Generate and download music TXT file with the loaded Model (MIDI.py v.6.7)\n",
        "number_of_tokens_to_generate = 2048 #@param {type:\"slider\", min:0, max:32768, step:128}\n",
        "creativity_temperature = 0.85 #@param {type:\"slider\", min:0.05, max:4, step:0.05}\n",
        "top_k_prob = 2 #@param {type:\"slider\", min:0, max:50, step:1}\n",
        "input_promt = \"SONG\" #@param {type:\"string\"}\n",
        "\n",
        "%cd /content/\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "context = input_promt\n",
        "x = torch.tensor([train_dataset.stoi[s] for s in context], dtype=torch.long)[None,...].to(trainer.device)\n",
        "y = sample(model, x, number_of_tokens_to_generate, temperature=creativity_temperature, sample=True, top_k=top_k_prob)[0]\n",
        "completion = ''.join([train_dataset.itos[int(i)] for i in y])\n",
        "print('Done! Saving output.txt!')\n",
        "with open(\"/content/output.txt\", \"w\") as text_file:\n",
        "    print(completion, file=text_file)\n",
        "\n",
        "\n",
        "from google.colab import files\n",
        "files.download('/content/output.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22qDcd4NO2bs",
        "cellView": "form"
      },
      "source": [
        "#@title Convert to MIDI from TXT (MIDI.py v.6.7)\n",
        "number_of_ticks_per_quarter = 201 #@param {type:\"slider\", min:1, max:1000, step:8}\n",
        "\n",
        "import MIDI\n",
        "import tqdm.auto\n",
        "\n",
        "song_score = []\n",
        "\n",
        "with open('/content/output.txt', 'r') as file:\n",
        "    input_string=file.read()\n",
        "\n",
        "input_string = input_string.split(\" \")\n",
        "if debug: print(input_string)\n",
        "\n",
        "\n",
        "i=0\n",
        "notes_specs = []\n",
        "song_name = ''\n",
        "z=1\n",
        "\n",
        "\n",
        "zero_marker = True\n",
        "song_score = [number_of_ticks_per_quarter, [['track_name', 0, b'Composed by Artificial Intelligence Model'], ['patch_change', 0, 0, 0], ['patch_change', 0, 3, 40]]]\n",
        "\n",
        "for i in tqdm.auto.tqdm(range(len(input_string))): \n",
        "  input_string_len = len(input_string[i])\n",
        "\n",
        "  if input_string[i].split('=')[0] == 'S':\n",
        "    try:\n",
        "      song_name = input_string[i].split('=')[1]\n",
        "      song_score.append([['track_name', 0, song_name]])\n",
        "    except:\n",
        "      print('Unknown Song name format', song_name)\n",
        "  if input_string[i].split('=')[0] == 'C':\n",
        "    try:\n",
        "      start_time = int(input_string[i].split('=')[1].split('-')[0])\n",
        "      duration = int(input_string[i].split('=')[1].split('-')[1]) \n",
        "      channel = int(input_string[i].split('=')[1].split('-')[2])\n",
        "      velocity = int(input_string[i].split('=')[1].split('-')[3])\n",
        "    except:\n",
        "      print('Unknown Chord:', input_string[i])\n",
        "\n",
        "  if str(input_string[i].split('=')[0]).split('-')[0] == 'N':    \n",
        "    try:\n",
        "      for x in range(len(str(input_string[i].split('=')[0]).split('-'))-1):\n",
        "        notes_specs = str(input_string[i].split('=')[0]).split('-')[x+1]\n",
        "        song_score[-1].append(['note', \n",
        "                              int(start_time), #Start Time\n",
        "                              int(duration), #Duration\n",
        "                              int(channel), #Channel\n",
        "                              int(notes_specs), #Note\n",
        "                              int(velocity)]) #Velocity              \n",
        "    except:\n",
        "      print(\"Unknown Notes: \" + input_string[i])\n",
        "\n",
        "midi_data = MIDI.score2midi(song_score)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with open('output.mid', 'wb') as midi_file:\n",
        "    midi_file.write(midi_data)\n",
        "    midi_file.close()\n",
        "print('Done!')\n",
        "\n",
        "from google.colab import files\n",
        "files.download('/content/output.mid')\n",
        "\n",
        "MIDI.score2stats(song_score)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJIcrVY6o9Gu",
        "cellView": "form"
      },
      "source": [
        "#@title Analysis of the IO MIDI Datasets\n",
        "MIDI_DIR = \"/content/output.mid\"\n",
        "### https://github.com/brennan2602/FYP\n",
        "\n",
        "#This file reads in the midi files in a directory, converts them to a string representation\n",
        "#when in a string representation it then gathers some statistics about the structure of the song\n",
        "import glob\n",
        "def get_piano_roll(midifile):\n",
        "\tmidi_pretty_format = pretty_midi.PrettyMIDI(midifile)\n",
        "\tpiano_midi = midi_pretty_format.instruments[0] # Get the piano channels\n",
        "\tpiano_roll = piano_midi.get_piano_roll(fs=20)\n",
        "\treturn piano_roll\n",
        "\n",
        "#uses split encoding scheme (here only encoding the note values)\n",
        "#works by looping through time increments of the piano roll array and writing the notes being played\n",
        "#at a given time sample as a number on the corresponding line of a string # is written when no notes played for that\n",
        "#sample\n",
        "def encode(arr):\n",
        "    timeinc=0\n",
        "    outString=\"\"\n",
        "    for time in arr:\n",
        "        notesinc = -1\n",
        "        #print(time)\n",
        "        if np.all(time==0):\n",
        "            outString=outString+\"#\"\n",
        "        for vel in arr[timeinc]:\n",
        "            notesinc=notesinc+1\n",
        "            if vel != 0:\n",
        "                noteRep=str(notesinc) + \" \"\n",
        "                #print(noteRep)\n",
        "                outString=outString+noteRep\n",
        "        outString=outString+\"\\n\"\n",
        "        timeinc = timeinc+1\n",
        "    return outString\n",
        "\n",
        "\n",
        "def getSilences(test):\n",
        "    test=test[:-1] #removing last line in string (always blank)\n",
        "    output=test.split(\"\\n\") #splitting into array\n",
        "    res = len(output)\n",
        "    #initialising counters\n",
        "    maxcounter=0\n",
        "    counter=0\n",
        "    silenceCount=0\n",
        "\n",
        "    for x in output:\n",
        "        if x == \"#\": #when a \"#\" is seen nothing is being played that sample\n",
        "            counter=counter+1 #this tracks a streak of silences\n",
        "            silenceCount+=1 #this tracks total silences\n",
        "        if x != \"#\":\n",
        "            counter=0 #reseting streak\n",
        "        if counter>maxcounter:\n",
        "            maxcounter=counter #updating longest silence streak when appropriate\n",
        "    return maxcounter,silenceCount\n",
        "\n",
        "\n",
        "#by looking at the length of song and the amount of silences this returns % silence\n",
        "def getPercentSilence(gen,silences):\n",
        "    test = gen\n",
        "    test = test[:-1]\n",
        "    output = test.split(\"\\n\")\n",
        "    res = len(output)\n",
        "    percent=silences/res\n",
        "    return percent\n",
        "\n",
        "\n",
        "def getStatsNotes(test):\n",
        "    test=test[:-1] #get rid of blank line at the end\n",
        "    notes=[]\n",
        "    output = test.split(\"\\n\") #split string on new lines\n",
        "\n",
        "    #initial values updated while looping through\n",
        "    maxPerSamp=0\n",
        "    silenceSamp=0\n",
        "    notesPlayed=0\n",
        "    maxNotes=0\n",
        "    maxVal=0\n",
        "    minVal=127\n",
        "\n",
        "    for x in output:\n",
        "        samp=x.split(\" \")\n",
        "        samp=samp[:-1] #theres a blank result at the end of array from split this indexing removes it\n",
        "        while \"0\" in samp:\n",
        "            samp.remove(\"0\") #sometimes 0 samples exist this removes them as they aren't notes played\n",
        "        if len(samp)==0:\n",
        "            silenceSamp+=1 #counting silences\n",
        "        notesPlayed=notesPlayed+len(samp) #counting notes played\n",
        "        if len(samp)>0:\n",
        "            #getting max and min note values at this time step\n",
        "            minimum=min(samp)\n",
        "            maximum=max(samp)\n",
        "            #updating max and min values note values for song if appropriate\n",
        "            if int(minimum)<minVal:\n",
        "                minVal=int(minimum)\n",
        "            if int(maximum)>maxVal:\n",
        "                maxVal=int(maximum)\n",
        "        #updating maximum number of notes per sample if appropriate\n",
        "        if len(samp)>maxNotes:\n",
        "            maxNotes=len(samp)\n",
        "    rangeNotes=maxVal-minVal #spread of notes\n",
        "    avgNotes = notesPlayed / len(output) #average notes per sample\n",
        "    adjNotes=notesPlayed /(len(output)-silenceSamp) #average notes per sample adjusted to remove silent samples\n",
        "    return rangeNotes, maxVal, minVal,maxNotes,avgNotes,adjNotes\n",
        "\n",
        "\n",
        "files=glob.glob(MIDI_DIR)#point towards directory with midi files (here same folder)\n",
        "print(files)\n",
        "\n",
        "for f in files:\n",
        "    print(f)\n",
        "    pr = get_piano_roll(f) #gets piano roll representation of the midi file\n",
        "    arr = pr.T\n",
        "    outString= encode(arr) #gets a string representation of the midi file\n",
        "    maxsilences, silences = getSilences(outString) #by passing in the encoded string get longest silence and the total\n",
        "                                                   #number of samples which are silent\n",
        "    noteRange, maxVal, minVal, maxNotes, avgNotes, adjAvg =getStatsNotes(outString) # getting some stats by looping\n",
        "                                                                                    # through encoded data\n",
        "    percentSilence= getPercentSilence(outString,silences) # get % silence from silence / outString length\n",
        "\n",
        "    #printing out to the user\n",
        "    print(\"longest silence is \",maxsilences,\"samples long\")\n",
        "    print(\"silence covers:\",round(percentSilence,4),\"%\")\n",
        "    print(\"notes span range:\",noteRange)\n",
        "    print(\"max note value:\",maxVal)\n",
        "    print(\"min note value:\",minVal)\n",
        "    print(\"average number of notes per sample:\",round(avgNotes,4))\n",
        "    print(\"average number of notes per sample (adjusted to remove silence samples):\",round(adjAvg,4))\n",
        "    print(\"max number of notes played in a sample:\",maxNotes)\n",
        "    print(\"\\n\")\n",
        "\n",
        "#NOTE some minor discrepencies vs reading in from generated file directly\n",
        "#However this does provide a uniform check to use for songs generated by both encoding schemes\n",
        "#Can also be used to evaluate training file\n",
        "#uses split encoding to get the text representation for ease of development"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xl1d9LYuJ9uZ",
        "cellView": "form"
      },
      "source": [
        "#@title Plot, Graph, and Listen to the Output :)\n",
        "graphs_length_inches = 18 #@param {type:\"slider\", min:0, max:20, step:1}\n",
        "notes_graph_height = 6 #@param {type:\"slider\", min:0, max:20, step:1}\n",
        "highest_displayed_pitch = 92 #@param {type:\"slider\", min:1, max:128, step:1}\n",
        "lowest_displayed_pitch = 24 #@param {type:\"slider\", min:1, max:128, step:1}\n",
        "\n",
        "midi_data = pretty_midi.PrettyMIDI('/content/output.mid')\n",
        "\n",
        "def plot_piano_roll(pm, start_pitch, end_pitch, fs=100):\n",
        "    # Use librosa's specshow function for displaying the piano roll\n",
        "    librosa.display.specshow(pm.get_piano_roll(fs)[start_pitch:end_pitch],\n",
        "                             hop_length=1, sr=fs, x_axis='time', y_axis='cqt_note',\n",
        "                             fmin=pretty_midi.note_number_to_hz(start_pitch))\n",
        "\n",
        "\n",
        "\n",
        "roll = np.zeros([int(graphs_length_inches), 128])\n",
        "# Plot the output\n",
        "\n",
        "#track = Multitrack('/content/output.mid', name='track')\n",
        "plt.figure(figsize=[graphs_length_inches, notes_graph_height])\n",
        "#fig, ax = track.plot()\n",
        "#fig.set_size_inches(graphs_length_inches, notes_graph_height)\n",
        "plt.figure(figsize=[graphs_length_inches, notes_graph_height])\n",
        "ax2 = plot_piano_roll(midi_data, int(lowest_displayed_pitch), int(highest_displayed_pitch))\n",
        "plt.show(block=False)\n",
        "\n",
        "\n",
        "FluidSynth(\"/content/font.sf2\", 16000).midi_to_audio('/content/output.mid', '/content/output.wav')\n",
        "Audio('/content/output.wav', rate=16000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Snu3fb4N-Nd"
      },
      "source": [
        "## Congrats! :) You did it :)"
      ]
    }
  ]
}